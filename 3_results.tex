\chapter{Results and Discussion}
In this chapter, we describe and discuss the results of this project.
In first section, we present implementation of the pipeline allowing us to transform the raw protein sequences from FASTA format into the input format of the neural network.
Then, in the second section, we reveal the networks used to predict the distograms of the proteins.
We also discuss the experiments done to confirm the correctness of our implementations and compare different architectures.
The last section is dedicated to constructing the 3D structure from these predicted distograms.

\section{Pipeline}
The pipeline was constructed to formalize and automatize the process of data preparation for the neural networks.
It was written in Snakemake workflow management system \cite{koster2012snakemake}, which is highly popular in the bioinformatics field with approximately 3 new citations each week.
The workflows consists of rules, which define the data dependency in form of rule input files, rule output files and the script, which needs to be executed to get from input to output. 
This definitions make using of Snakemake very convenient.
The rules are defined as a subset of YAML, extended by the option to run python code directly.
This makes them easily readable by humans.
The structuring of the workflow allows seamless scalability to server environments and parallelization to multiple cluster nodes and multiple cores.
This capability was very crucial for this project due to the amount of data available.
Furthermore, Snakemake automatically checks for completed rules, which is in turn used to run only the rules needed for the defined target.
It also check the exit code of the scripts and informs user if there is an error during the execution and therefore decreases a chance of working with faulty data to the minimum.
Finally, conda \cite{conda}, the package management system, is nicely integrated into Snakemake, which simplifies the installation of most packages and bioinformatics tools needed during the analysis.
It is also possible to fix package versions used during the analysis, which hugely improves reproducibility.
Next, we present the main steps in the pipeline.
The complete pipeline can be found on our GitHub repository \cite{github}.

\subsection{PSI-BLAST}
The first step in the pipeline was the Position-Specific Iterated BLAST \cite{altschul1997gapped}.
It is a variation of the BLAST algorithm focused on protein search

% https://www.youtube.com/watch?v=IXuzKjCm0ME seems like a good source
% for blast part
% BLAST is a heuristic that attempts to optimize a specific similarity measure.
% The original BLAST program seeks short word pairs whose aligned score is at least T. Each such ‘hit’ is then extended, to test whether it is contained within a high-scoring alignment. For the default T value, this extension step consumes most of the processing time. 

\subsection{HHblits}
% fasta input -> msa out
\cite{remmert2012hhblits}

\subsection{plmc}
% msa out -> potts
% L-BFGS 
% 500 iteration
\cite{plmc}

\subsection{input creation}


%\subsection{Distances}
%\subsection{Structure Realization}

%\subsection{Evaluation and Visualization}

%\begin{itemize}
%    \item TM score, GDT\_TS
%\end{itemize}
    
\section{Inter-residue distance modelling}

\subsection{Neural Networks}
% architectures (convnet, Alphafold, Inception), training, auxiliary losses, ...

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/Architectures.png}
    \caption{Neural Network architectures used in training. a) visualizes the diagram of the entire network; b) shows the AlphaFold module, c) is a Inception Resnet module (inspired by Inception module used in GoogleNet) and d) is a simple Convolutional Network}
    \label{fig:architectures}
\end{figure}

Number of parameters:

AlphaFold\_A = 
AlphaFold\_T = 10,745,464
Inception = 1,495,416
ConvNet = 6,654,328

\subsection{Cropping}

\subsection{Evaluation}

% distance maps (loss, lddt), Q3 ss accuracy, 


% \subsection{Overtraining of the model}
% We performed an experiment, to observe how our model behaves with different kinds of data.
% First, we initialized the model parameters randomly.
% Our primary expectation was that, if we use also randomly generated input and no training, the output will look very randomly as well.
% % This expectation is visualized in the Figure (\ref{fig:in_out_dep1}).
    
% % \begin{figure}
% %     \centering
% %     \includegraphics{}
% %     \caption{Caption}
% %     \label{fig:in_out_dep1}
% % \end{figure}
    
% Surprisingly, this was not the case in the experiment.

% \begin{itemize}
%     \item Architecture (Blocks, dilated convolutions, groups)
%     \item Number of parameters
%     \item Projections (Up, Down) - Conv1x1
%     \item Auxiliary losses
% \end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_andy/error_distribution_test_set_ensemble_simetrized.png}
    \caption{Error distribution}
    \label{fig:error_dist}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_andy/error_length_test_set_ensemble_simetrized.png}
    \caption{Error - length relationship}
    \label{fig:err_len}
\end{figure}

\begin{figure}
    \centering
    % \includegraphics[width=0.5\linewidth]{imgs_andy/distograms/test_vis_sim_best_4i4tE01_7.13.png}
    \includegraphics[width=0.75\linewidth]{imgs_andy/distograms/test_vis_sim_5perc_1vmgA00_20.42.png}
    \includegraphics[width=0.75\linewidth]{imgs_andy/distograms/test_vis_sim_median_4mveA00_28.69.png}
    \includegraphics[width=0.75\linewidth]{imgs_andy/distograms/test_vis_sim_95perc_2l1iA00_39.11.png}
    % \includegraphics[width=0.5\linewidth]{imgs_andy/distograms/test_vis_sim_worst_3m4yA01_71.41.png}
    \caption{Comparison of ground truth and predicted distograms. We selected three proteins based on the calculated error made in their predictions. First row is a very good prediction of a protein 1vmgA00 with measured error 20.42. Only 5\% of the distograms achieved better accuracy. Second row is a prediction of protein 4mveA00 with median error amongst the test set. Third row is a prediction of protein 2l1iA00. 95\% of the errors from test set achieved better score. In all cases, the ground truth is displayed on the left and the predicted distograms on the right.}
    \label{fig:distograms}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_andy/139lA00_real_pred_3D_v2.png}
    \caption{Comparison of real vs. predicted 3D structure}
    \label{fig:real_pred}
\end{figure}

\section{Structure Realization}

\subsection{Gradient Descent Based Structure Realization}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/inter_data.png}
    \caption{Inter-atom distance distributions (top row) and angles defined by three consecutive atoms in a plane (bottom row) calculated from 50 protein structure downloaded from PDB. The red line represents the mean of the distribution, together with the first number in red text. The second number is the standard deviation}
    \label{fig:interresidue}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/norm_histograms.png}
    \caption{Fitted normal distributions to the distance histograms (middle) and von Mises distributions to the angle histograms (right) to a small region of domain 16pkA01}
    \label{fig:distributions}
\end{figure}

In order to implement the model of protein geometry, one has to treat the distances between neighboring atoms in the backbone as constants, together with the angles of 3 consecutive backbone atoms. We analyzed coordinates of 50 randomly selected proteins and calculated these distances and angles; the results are shown in Figure \ref{fig:interresidue}. The means of the distributions were used in the creation of the protein geometry.

For a given structure we generated 1000 unique random initial states by sampling from the von Mises distributions fitted to the histograms. The individual distributions are depicted on Figure \ref{fig:distributions}. From each angle distribution (right of the Figure \ref{fig:distributions}) one value was picked. The normal distance distributions (Figure \ref{fig:distributions} - middle) were used for the calculation of the loss function - see Equation \ref{eq:dist_pot}.

For each torsion angle initiation we let the algorithm run for 1000 iterations. However, after every 200 iterations the algorithm picked the best structure (according to the loss) and continued the optimization with decreased learning rate (by a factor of 10).

The initial parameters were chosen by visually exploring the learning curves of 3 random states. The grid search was composed of combination of parameters:

$$learning~rate = [0.1, 0.05, 0.01. 0.005, 0.001]$$
$$momentum = [0.1, 0.3, 0.5, 0.7, 0.9]$$


\subsection{Inter-residue Contacts + Secondary structure based Structure Realization}

\subsection{Evaluation}

\section{Comparison with other methods}
\subsection{Distances}
\subsection{Structures}

\section{Potential improvements}

\subsection{Regarding Input Data}
what to do when the protein does not have enough similar sequences in the database?
NEFF

PLMC might not converge. What are the reasons for this and how should we handle it.

Optimization of the input pipeline is needed.
HHblits and Psiblast both do a sequence search and MSA.
Ideally we would like to do this just once and then from MSA compute the PSSM and potts models.
Although it might be beneficial to do it like it is, because the input features will be less correlated to each others.

It might be beneficial to include calculated limits of the physical distance of 2 aminoacids.
For example, we know that 2 aminoacids next to each other are 3.8A away.
Similarly, 2 aminoacids at position $i$ and $i+2$ are ???5.6???A away.
For position $i$ and $i+3$ this starts t be interesting, but calculating the maximum distance and the minimum distance should be relatively easy.

Refactoring pipeline to be truly reproducible.

Using more covariance data modelling approaches, not just PSSM and potts, but also PSICOV and Bayesian networks.

\subsection{Regarding Neural Networks}
Neural networks could be modified to predict parameters of some distribution of distances.
In currect setting we are predicting distrogram characterized by 32 numbers.
If we would be able to identify some reasonable parametric distribution for distances, these could be used instead of the distrogram, reducing the number of parameters of the network and making the formulation closer to some theorethical formulation related to protein than very general.

Use accumulating gradients. This would allow to train on bigger batches than is the memory limit of the GPU and also would allow to train on multiple GPUs.
% https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255

\subsection{Regarding Structure Realization}

Even though the algorithm for optimizing structure works, it is highly dependant on the choice of hyperparameters, which might not be ideal for every domain. The authors of AlphaFold did not use gradient descent for optimization but a more advanced method - L-BFGS (Limited memory Broyden–Fletcher–Goldfarb–Shanno) which is a method which resembles the second order optimization by estimating the Hessian matrix (matrix of second derivatives). The method does not pick directions where the loss function changes the fastes, but rather where the gradients change the fastest. This results in more reasonable especially and the convergence is usually reached in smaller number of steps. The biggest downside of the method is, that it is slow and tends to get stuck in saddle points of the loss function \cite{nn_dl}.