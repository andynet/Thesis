\chapter{Materials and Methods}

In this section we describe the steps required to get from a primary protein sequence to its tertiary structure.
It begins with the description of the input and the motivation for using it. 
Second part focuses on the model choices and mainly describes the convolutional neural network. 
The last section focuses on the structure realization. 
A graphical overview of the entire pipeline is showed in Figure \ref{fig:project_pipeline}. 
In each of the 3 subsections the corresponding part of the diagram will be unraveled to accompany the verbal descriptions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/Project_pipeline_small.png}
    \caption{Pipeline overview}
    \label{fig:project_pipeline}
\end{figure}

\section{Primary Sequence to CNN Input}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/pipeline_input.png}
    \caption{Input / Output data generation}
    \label{fig:pipeline_input}
\end{figure}

During evolution, we expect that functional parts of proteins are going to be conserved. 
Furthermore, we expect that two amino acids that are close to each other in the 3D space (not necessarily in the primary sequence) are going to co-evolve as well. 
This co-evolution of spatially proximal amino acids should be reflected in the Multiple Sequence Alignment (MSA) of a protein. 
In order to generate a MSA for a selected domain, one has to first find proteins that share some ancestry with it (homologs), which is done by searching through database of sequences and picking proteins that have certain sequence similarity. 
From these sequences we can generate the MSA.
    
From the MSA we can calculate the propensities of amino acids at each position of the sequence very easily, which yields the Position Specific Scoring Matrix (PSSM). 
To model the interaction we can use the Direct Coupling Analysis (Potts model).
    
This section is going to be organised as follows. 
We are first going to explain how to search for similar proteins and construct the MSA using the HHblits tool. 
The second part will focus on the MSA summary statistics such as PSSMs and Potts Models. 
In the last part we will discuss the encoding of the input, so that it is ready for use in the convolutional neural network.
    
% \subsection{Available data}
% % \section{Data used for training and model evaluation}
    
% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=\linewidth]{imgs_tomas/cath.png}
%     \caption{CATH hierarchy \cite{cath}}
%     \label{fig:cath}
% \end{figure}
    
% For training we used protein sequences from the CATH S35 dataset. 
% CATH is a database that clusters proteins with known structures based on 4 criteria: Class (secondary structure classes), Architecture (secondary structure arrangement in 3D space), Topology (how secondary structure elements are connected) and Homologous Superfamily (evolutionary relationship between domains). 
% Figure \ref{fig:cath} shows the hierarchical relationship between the classes.
    
% To ensure dataset diversity, we used a dataset of sequences with pairwise sequence similarity of at most 35\% (hence the S35). 
% The list of domains together with their sequences in \texttt{fasta} format can be accessed at: \href{ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/sequence-data/cath-domain-seqs-S35.fa}{oregonftp.biochem.ucl.ac.uk}.
    
% Domains are short regions of proteins that fold more or less independently of each other. 
% The \texttt{fasta} file contains for each domain its protein name, chain identifier and the domain ranges. 
% This information is crucial for downloading and preparing the structures from Protein Data Bank database. 
% Two example headers are shown below. 
    
% \begin{center}
%     \texttt{>cath|4\_2\_0|1a41A02/217-310}\\
%     \texttt{>cath|4\_2\_0|3lnnA01/12-42\_283-342}
% \end{center}
    
% These two examples show domains - \texttt{1A41} and \texttt{3LNN}, downloaded from the CATH database version 4.2.0. 
% The next letter (\texttt{A} in both cases) represents the chain and the last two characters are the domain id, since a chain can consist of several domains. 
% The full dataset (version from Sept. 4 2017) consists of 31289 domains. 
    
% We decided to exclude segmentated domains from the dataset and also the ones with missing PDB coordinates. 
% This resulted in a set of 19953 domains. 
% Looking at the sizes of homologous superfamilies in Figure \ref{fig:cath}, there are few large ones (like the immunoglobulin family in the bottom right corner with more than 8000 domains) and the rest are relatively small (less than 50). 
% Our model works woth evolutionary data (MSA - PSSMs/Potts), which means that having a overrepresented family in the data set might introduce some biases that might make the model generalization more difficult. 
% For this reason we decided to impose an exclusion threshold (= 500), where in families with more members we randomly picked "exclusion threshold" of domains out of them. 
% After this filtering, the dataset size reduced to 11014 domains.
% The distributions of CATH classes can be seen in Figure \ref{fig:cath_filtered}.
    
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.4]{imgs_tomas/cath_distributions_filtered.png}
%     \caption{Caption}
%     \label{fig:cath_filtered}
% \end{figure}

\subsection{Sequence search and MSA construction}

% for blast part
% BLAST is a heuristic that attempts to optimize a specific similarity measure.
% The original BLAST program seeks short word pairs whose aligned score is at least T. Each such ‘hit’ is then extended, to test whether it is contained within a high-scoring alignment. For the default T value, this extension step consumes most of the processing time. 

% At the bottom of the input preparation pipeline lies the sequence search step. The goal of an algorithm is to efficiently search large databases of sequences and picking ones that satisfy some similarity measure.

% The first method which we used - PSIBlast, iteratively constructs Position Specific Scoring Matrices which are used to compute score of candidate sequences (more on PSSMs in the subsection below). If the score exceeds some threshold the candidate sequence is added to the pool, from which the PSSM for the next iteration is computed. In the first iteration the sequences are compared using amino acid substitution matrix (eg. BLOSUM62).  

\subsection{Potts Models}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/potts_example.png}
    \caption{Potts model example for an MSA shown in the upper left corner and the exact parameters calculated. In the bottom is a plot of probabilities for each unique sequence consisting of 3 symbols, calculated using the potts model formula \ref{eq:Potts}}
    \label{fig:potts_example}
\end{figure}

The goal of a Potts Model is to simulate probability distribution of every possible combination of symbols in an alphabet

Lets assume that we have a sequence $\boldsymbol{\sigma} = (\sigma_1, \sigma_2, ..., \sigma_L)$, where $L$ is the length of the multiple sequence alignment. 
Lets further assume that each symbol $\sigma_i$ in the sequence comes from an alphabet $\mathcal{A} = (a_1, a_2, ..., a_Q)$ of size $Q$. 
Thus for sequence length $L$ we can generate $Q^L$ number of unique sequences.
    
In our case, the sequence $\boldsymbol{\sigma}$ is some sequence of amino acids, and thus the alphabet size is equal $Q = 21$ (20 amino acids, 1 gap symbol).
        
Given a MSA, we would like to find pairs of amino acids that are coupled, ie. the pairs of amino acids that are correlated due to their direct contact. 
The reason why we can not use correlation data directly, is that some pairs of amino acids might be correlated through some intermediate paths.
        
We have to keep in mind that MSA is only generated from a sample of sequences. 
Thus it can only help us to estimate the frequencies and connected correlations of amino acids. 
To calculate the connected correlation of a pair of amino acid, we can use the covariance formula:
            
$$c_{ij}(k, l) = f_{ij}(k, l) - f_i(k) f_j(l)$$
        
where
$$f_i(k) = \frac{1}{B} \sum_{i = 1}^B \delta(\sigma_i^{(b)} = k)$$
$$f_{ij}(k, l) = \frac{1}{B} \sum_{i = 1}^B \delta(\sigma_i^{(b)}, k)\delta(\sigma_j^{(b)}, l)$$
        
calculate the frequency of symbol $(k)$ and pair of symbols $(k, l)$ at position $(i)$ and positions $(i, j)$ in the MSA. 
$\delta(\sigma_i^{(b)}, k)$ is an indicator function that is equal to one when $\sigma_i^{(b)} = k$.
        
% add example
        
Our goal is to create a model: $\mathcal{P(\bm{\sigma})}$ that can reproduce the observed frequencies. 
The constraints of the model are:
        
$$\mathcal{P}(\sigma_i = k) = \sum_{\substack{\bm{\sigma}\\ \sigma_i = k}} \mathcal{P}(\bm{\sigma}) = f_i(k)$$
$$\mathcal{P}(\sigma_i = k, \sigma_j = l) = \sum_{\substack{\bm{\sigma}\\ \sigma_i = k \\ \sigma_j = l}} \mathcal{P}(\bm{\sigma}) = f_{ij}(k, l)$$
        
The model that satisfies these constraints and has the maximal information entropy is called a Potts Model:
        
\begin{equation}
    \mathcal{P}(\bm{\sigma}) = \frac{1}{\mathcal{Z}} exp\left(\sum_{i = 1}^{L-1} \sum_{j=i+1}^L \bm{J}_{ij}(\sigma_i, \sigma_j) + \sum_{i=1}^L \bm{h}_i({\sigma_i})\right)
    \label{eq:Potts}
\end{equation}
    
In this equation, $\mathcal{Z}$ is the normalizing constant/partition function that ensures that the probabilities sum to 1. 
The parameters of the model $\bm{J}$ and $\bm{h}$ describe the pairwise interactions and propensities respectively.
        
To find the parameters of a model that best describe the observed MSA, we can define a loss function:
        
\begin{equation}
    \mathcal{L} = -\frac{1}{B} \sum_{i=1}^B logP(\bm{\sigma}^{(b)})
    \label{eq:potts_loss}
\end{equation}
        
Specifically for Potts Model: 
        
\begin{equation}
    \mathcal{L}_(\bm{h}, \bm{J}) = log \mathcal{Z} - \sum_{i = 1}^L \sum_{k \in \mathcal{A}} f_i(k)\bm{h}_i(k) - \sum_{i = 1}^{L-1} \sum_{j=i+1}^L \sum_{k, l \in \mathcal{A}} f_{ij}(k, l) \bm{J}_{ij}(k, l)
    \label{potts_loss_ml}
\end{equation}
        
This loss function is differentiable and so we seek to find its minimum. 
However, to do so, we need to know the value of the partition function ($\mathcal{Z}$), which is practically incomputable for larger domains. To know its exact value, we would need to calculate the value of the potts model for every possible combination of letters in the alphabet for a given length of sequence. 
Therefore its approximation is required.
        
Instead of considering the entire space of possible sequences, we can ask about the probability of a certain symbol in a sequence in MSA ($\sigma_r^{(b)}$) knowing the rest of symbols in the sequence ($\bm{\sigma}_{\setminus r}^{(b)}$). 
The probability function in (\ref{eq:potts_loss}) becomes:
        
$$
P(\sigma_{r} = \sigma_r^{(b)} | \bm{\sigma}_{\setminus r} = \bm{\sigma}_{\setminus r}^{(b)}) = \frac{exp \left(h_r (\sigma_r^{(b)}) + \sum_{\substack{i = 1\\i\neq r}}^L J_{ri}(\sigma_r^{(b)}, \sigma_i^{(b)})\right)}{\sum_{q \in \mathcal{A}} exp \left(h_r(q) + \sum_{\substack{i = 1\\i\neq r}}^L J_{ri}(q, \sigma_i^{(b)})\right)}$$
        
This function is again, differentiable, although it does not minimize the loss defined in (\ref{eq:potts_loss}).
However, as opposed to \ref{potts_loss_ml}, this objective function can be computed in reasonable time with gradient descent techniques (either first order optimization or second order optimization approximation via algorithms such LBFG-S)\cite{potts1, potts2}.
        
\subsection{Position-specific Scoring Matrix}

Position-specific Scoring Matrix (PSSM) is generally used to represent biologically important patterns across multiple sequences. 

To create the PSSM, let us assume that we already constructed a multiple sequence alignment (MSA) of our target sequence and related proteins.
This multiple sequence alignment consists of $N$ rows representing sequences and $L$ columns representing positions of alignment.
Furthermore, each sequence is a sequence of letters from an alphabet with $Q$ symbols.

Then, the first step in creation of PSSM is the calculation of the Position Frequency Matrix (PFM), by simply counting the number of occurences of a symbol at a position in the MSA.
Formally, PFM is a matrix with $Q$ rows and $L$ columns, where each entry $\PFM_{q, l}$ is given by Equation \ref{eq:pfm}.

\begin{equation}
    \PFM_{q, l} = \sum_{n = 1}^{N} \delta(\MSA_{n, l}, q)
    \label{eq:pfm}
\end{equation}

From the PFM, a Position Probability Matrix (PPM) is then created simply by scaling the counts by the number of sequences in the MSA.
Therefore:

\begin{equation}
    \PPM_{q, l} = \frac{1}{N} \PFM_{q, l}
\end{equation}

The PSSM is then defined as the log likelihoods of the PPM transformed by some background model $b$.
For example, the simplest background model might be the one, where each symbol appears in the sequences equally likely, thus $b_q = \frac{1}{Q}$.
This definition gives us the Equation \ref{eq:pssm}.

\begin{equation}
    \PSSM_{q, l} = \log_2 \frac{\PPM_{q, l}}{b_q}
    \label{eq:pssm}
\end{equation}

In practice, more complex background models, such as the total frequency of a symbol in the entire MSA, is often used.
Moreover, it is often encountered, especially when number of sequences in the MSA is small, that some entries of the PFM are zeroes.
This is an issue, because it renders some sequences impossible even though they might be just very unlikely and due to the limited number of samples not observed.
To address this issue, pseudocounts are often used.
This means that we start the PFM construction with one assigned to every entry instead of zero.
It will ensure that the probabilities in the PPM will be non-zero and it also accounts for finite sample size.


% Position Probability Matrix $\mathcal{F} = \{f_{ij}\}$ with $L$ rows (target sequence length) and $Q$ columns (alphabet size). 
% Each entry in the matrix is a frequency of occurence of a symbol in the current position in the MSA:
        
% $$f_i^*(k) = \frac{1}{B} \sum_{b = 1}^B \delta(\sigma_i^{(b)}, k)$$

% In both cases,  the  influence  of  pseudocounts  declines  with  the  size  of  the  training  set  (√N/N  in  the  first  case,  0.1/N  in  the  other),  which  is  just  what  you  would  want,  since  the  purpose  of  pseudocounts  is  to  diminish  the  distortions  inherent  in  using  a  small  training  set.  The  higher  the  value  of  B,  the  more  sequences  will  be  found  (including  perhaps  some  real  binding  sites  that  might  otherwise  be  missed)  but  at  the  cost  of  diluting  the  impact  of  what  is  known  about  the  binding  site.  Lower  values of B thus produce fewer false positives.
        
% To ensure that there are no zero probabilities in the final matrix, pseudocounts are added. 
% If $u_k$ is the occurence of a symbol in the entire MSA and $C$ is some arbitrary constant (usually equal to $\sqrt{L}$ or $0.1$), the frequency can be calculated as:
% $$f_i(k) = \frac{B(f_i^*(k) + u_k C)}{B + C}$$
        
% The items in PSSM are then defined as:
        
% $$PSSM_i(k) = log_2\left(\frac{f_i(k)}{u_k}\right)$$
        
% (\href{http://www.people.vcu.edu/~elhaij/IntroBioinf/Notes/PSSM.pdf}{http://www.people.vcu.edu/~elhaij/IntroBioinf/Notes/PSSM.pdf})
        
% (\href{https://en.wikipedia.org/wiki/Position_weight_matrix#cite_note-Stormo1982-1}{WIKI})
        
% \section{Data used for training and model evaluation}
    
% \begin{figure}[b!]
%     \centering
%     \includegraphics[width=\linewidth]{imgs_tomas/cath.png}
%     \caption{CATH hierarchy \cite{cath}}
%     \label{fig:cath}
% \end{figure}
    
% For training we used protein sequences from the CATH S35 dataset. 
% CATH is a database that clusters proteins with known structures based on 4 criteria: Class (secondary structure classes), Architecture (secondary structure arrangement in 3D space), Topology (how secondary structure elements are connected) and Homologous Superfamily (evolutionary relationship between domains). 
% Figure \ref{fig:cath} shows the hierarchical relationship between the classes.
    
% To ensure dataset diversity, we used a dataset of sequences with pairwise sequence similarity of at most 35\% (hence the S35). 
% The list of domains together with their sequences in \texttt{fasta} format can be accessed at: \href{ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/sequence-data/cath-domain-seqs-S35.fa}{oregonftp.biochem.ucl.ac.uk}.
    
% Domains are short regions of proteins that fold more or less independently of each other. 
% The \texttt{fasta} file contains for each domain its protein name, chain identifier and the domain ranges. 
% This information is crucial for downloading and preparing the structures from Protein Data Bank database. 
% Two example headers are shown below. 
    
% \begin{center}
%     \texttt{>cath|4\_2\_0|1a41A02/217-310}\\
%     \texttt{>cath|4\_2\_0|3lnnA01/12-42\_283-342}
% \end{center}
    
% These two examples show domains - \texttt{1A41} and \texttt{3LNN}, downloaded from the CATH database version 4.2.0. 
% The next letter (\texttt{A} in both cases) represents the chain and the last two characters are the domain id, since a chain can consist of several domains. 
% The full dataset (version from Sept. 4 2017) consists of 31289 domains. 
    
% We decided to exclude segmentated domains from the dataset and also the ones with missing PDB coordinates. 
% This resulted in a set of 19953 domains. 
% Looking at the sizes of homologous superfamilies in Figure \ref{fig:cath}, there are few large ones (like the immunoglobulin family in bottom left with more than 8000 domains) and the rest are relatively small (less than 50). 
% Our model works woth evolutionary data (MSA - PSSMs/Potts), which means that having a overrepresented family in the data set might introduce some biases that might make the model generalization more difficult. 
% For this reason we decided to impose an exclusion threshold (= 500), where in families with more members we randomly picked "exclusion threshold" of domains out of them. 
% After this filtering, the dataset size reduced to 11014 domains. 
% The distributions of CATH classes can be seen in Figure \ref{fig:cath_filtered}.
    
% \begin{figure}
%     \centering
%     \includegraphics[scale=0.4]{imgs_tomas/cath_distributions_filtered.png}
%     \caption{Caption}
%     \label{fig:cath_filtered}
% \end{figure}
    
%\subsection{Train Validation Test split}
    
% In the AlphaFold paper \cite{alphafold}, the authors decided to create train/validation/test set of homologous superfamilies instead of randomly picking the domain. 
% The rationale behind this is to include all members of each homologous superfamily in either training, validation or test set. 
% This ensures that when evaluating the model performance of a set of proteins, that very similar proteins were not used for training. 
    
% We decided to train our models on a set of 9394 domains (several had to be excluded due to factors such as low sequence similarity to known structures or inability to converge/slow convergence of the Potts Model). This set of domains was used to train the neural networks and for the validation during the training process.
% The performance of the different models was evaluated on a independent validation set of 1000 domains. For each model we evaluated its inter-residue distance predictions as well as the auxiliary outputs (secondary structure and phi and psi torsion angles).
% The final structures and performance metrics were evaluated on a set of 500 test domains.
    
% Furthermore, to simulate the famous CASP competition we downloaded the domains from CASP13 (2018) - regular targets category (T). This allowed us to compare our results with the best teams in the world.
% These domains were not used in training because the CATH domain list we downloaded was uploaded in September 2017. 

\input{2_neural_networks}

\section{Structure Realization}

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{imgs_tomas/backbone.png}
    \caption{Protein backbone with atom names. Modified picture from \cite{ramachandran}}
    \label{fig:backbone}
\end{figure}

The structure realization step refers to a process of optimizing three dimensional conformation of protein according to some constraints or energy/potential function. If we assume that the bond lenth between neighboring atoms in the backbone (atoms N, C$_\alpha$ and C) + C$_\beta$ atom is constant as well as the angle between atoms in the same plane (angles N-C$_{\alpha}$-C, C$_{\alpha}$-C-N, C-N-C$_{\alpha}$) then the entire structure can be fully described by a set of three torsion angles $\phi$, $\psi$ and $\omega$.

This assumption allows us to create a model of protein geometry $\mathcal{G}(\phi, \psi)$ that can be optimized ($\omega$ is almost always 180\degree, and so was treated as a constant). We can use the predicted distance histograms (distograms) together with predicted angle distributions to find a structure which inter-residue distances and torsion angles are as close as possible to the modes of these distributions.

In the following sections we will describe how we tackled the protein geometry problem, together with explanation of torsion angles and how to calculate atom coordinates from them and a short recap of basic linear algebra operations that were used. In the second part we will describe the optimization process.

\subsection{Torsion Angles to Coordinates}

% \begin{figure}
%     \centering
%     % \includegraphics[scale=0.27]{imgs_tomas/torsion.png}
%     \includegraphics[width=\linewidth]{imgs_tomas/torsion.png}
%     \caption{Torsion Angles - planes and inter-atom distance and angles notation}
%     \label{fig:torsion}
% \end{figure}

% Torsion angle, or a dihedral angle, is an angle defined by two intersecting planes. A protein is essentially composed of a sequence of aminoacids, where every aminoacid contains atoms N, C$_\alpha$ and C and almost all (except of glycine) contain a C$_\beta$ atom with a residue.
% The C$_\beta$ has a bond with the C$_\alpha$ atom - Figure \ref{fig:backbone} depicts the individual atoms of the backbone together with C$_\beta$ atom.

% Since there are three atoms in the backbone, we can define three pairs of intersecting planes and thus define the three torsion angles (Figure \ref{fig:torsion}). Knowing the position of three preceding atoms, the torsion angle $\phi$ defines the position of the next C atom, the torsion angle $\psi$ defines the position of the next N atom and finally $\omega$ defines the position of the next C$_\alpha$ atom. The bond between atom C and N in the protein backbone is a peptide bond that behaves similarly to a double bond and is almost always equal to 180\degree~(trans isomerism; 0\degree~for cis configuration).

Seeing that essentialy each torsion angle defines a position of an atom in relation to the previous three atoms in the structure, which are placed in a single plane, we can simplify the problem to follwing steps:

\begin{enumerate}
    \item Place the atom in the same plane as the previous three atoms, satisfying the inter-atom angles and distances (eg. for placing atom N$_1$, we need to know position of N$_0$, C$_{\alpha 0}$ and C$_0$ and angles NC$_\alpha$C and C$_\alpha$CN).
    
    \item Define a vector $\bm{v}$ as the difference between the coordinates of the new atom and the last atom in the structure (eg. $\bm{v} = $N$_1$ - C$_0$).
    
    \item Define a unit vector $\bm{k}$ around which the vector $\bm{v}$ will be rotated. This vector lies at the intersection of the two planes (eg. $\bm{k} = \frac{\textrm{C}_0 - \textrm{C}_{\alpha 0}}{d_{\textrm{C}_{\alpha} \textrm{C}}}$)
    
    \item Rotate the vector $\bm{v}$ around an axis with unit vector $\bm{k}$ and move it by adding the coordinates of the last atom in the structure
\end{enumerate}

These steps will be described in further detail in following section. Figure \ref{fig:bond_rotation} visualizes the steps except of the vector translocation.

% !!! REDO !!!
\begin{figure}
    \centering
    \includegraphics[scale=0.28]{imgs_tomas/rodrigues_backbone.png}
    \caption{Torsion angle rotation as vector rotation}
    \label{fig:bond_rotation}
\end{figure}

\subsection{Background - Vector Cross-product}

The cross product $\bm{v} \times \bm{w}$ of two 3D vectors is a vector pointing to a direction perpendicular to both vectors (decided by the right hand rule, where if $\bm{v}$  is represented by a thumb and $\bm{w}$ as index finger, than the direction is given by the middle finger). The size of this vector is equivalent to the area of the parallelogram with sites of sizes $||\bm{v}||$ and $||\bm{w}||$.

Formally the cross-product is a determinant of a special matrix with first column being the unit vectors $u_x$, $u_y$ and $u_z$ of the cartesian coordinate system and the rest are vectors $\bm{v}$ and $\bm{w}$:

$$\bm{v} \times \bm{w} = det
\left(
\begin{array}{ccc}
    u_x & v_{x} & w_{x}\\
    u_y & v_{y} & w_{y}\\
    u_z & v_{z} & w_{z}\\
\end{array}
\right) = ||\bm{v}||~||\bm{w}||~sin(\alpha)~\bm{n}
$$

where $\alpha$ is the angle between the two vectors and $\bm{n}$ is the unit normal vector perpendicular to both vectors.

\begin{figure}
    \centering
    \includegraphics[scale=0.23]{imgs_tomas/vector_ops.png}
    \caption{Visual demonstration of vector cross-product - a) and scalar/dot product - b)}
    \label{fig:vector ops}
\end{figure}

\subsection{Background - Vector Dot-product}
The dot product $\bm{k} \cdot \bm{v}$ is an operation that projects vector $\bm{v}$ onto a line defined by vector $\bm{k}$ and then scales the projection by the size of vector $\bm{k}$. In the special case when $\bm{k}$ is a unit vector, the result is a simple projection. Formally:

$$\bm{k} \cdot \bm{v} = ||\bm{k}||~||\bm{v}||~cos(\alpha) = k_x v_x + k_y v_y + k_z v_z$$

where $\alpha$ is the angle between the two vectors.

\subsection{Vector Rotation}

\begin{figure}
    \centering
    \includegraphics[scale=3]{imgs_tomas/rodrigues_3d2d.png}
    \caption{Caption}
    \label{fig:rodrigues3d2d}
\end{figure}

Lets assume that there is a vector $\bm{v}$ and a vector $\bm{k}$. Our task is to find vector $\bm{v_{rot}}$ which is a rotated version of vector $\bm{v}$ around a line defined by vector $\bm{k}$. Figure \ref{fig:rodrigues3d2d} depicts all the vectors and operations used in the following paragraphs.

The first step is to redefine the coordinate system in terms of 3 unit vectors: one is vector $\bm{k}$, second is a vector perpendicular to both $\bm{k}$ and $\bm{v}$: $\bm{n} = \frac{\bm{k} \times \bm{v}}{||\bm{k} \times \bm{v}||}$ and third a vector perpendicular to both $\bm{n}$ and $\bm{k}$: $\bm{m} = \bm{n} \times \bm{k}$.

\begin{enumerate}
    \item "$\bm{k}$ axis"
    
    The new rotated vector will always be the same distance away in the $\bm{k}$ vector direction. The amount is the projection of $\bm{v}$ onto $\bm{k}$, which is $\bm{k} \cdot \bm{v}$.
    
    \item "$\bm{n}$ axis"
    
    The amount by which we need to scale the unit vector $\bm{n}$ is the $sin(\theta)$ multiplied by the radius of the circle in the Figure \ref{fig:rodrigues3d2d}. The radius of the circle is a dot product of vector $\bm{m}$ and the initial vector $\bm{v}$.
    \item "$\bm{m}$ axis"
    
    The same reasoning as above, but this time the radius has to be multiplied by the cosine of $\theta$.
\end{enumerate}

All together, the rotational formula is:

\begin{equation}
    v_{rot} = \bm{k}(\bm{k} \cdot \bm{v}) + \bm{n} (\bm{m} \cdot \bm{v}) sin(\theta) + \bm{m} (\bm{m} \cdot \bm{v}) cos(\theta)
    \label{eq:rotformula}
\end{equation}

\subsection{Beta carbon atom placement}
% $C_{\beta}$

\begin{figure}
    \centering
    % \includegraphics[scale=0.25]{imgs_tomas/cbeta.png}
    \includegraphics[width=\linewidth]{imgs_tomas/cbeta.png}
    \caption{Caption}
    \label{fig:cbeta}
\end{figure}

To place the C$_{beta}$ we used the rotational formula \ref{eq:rotformula} but the vectors $\bm{v}$ and $\bm{k}$ had to be computed from the coordinates. If vector $\bm{v}_{C_\alpha N}$ is a unit vector in the same direction as the bond between the C$_\alpha$ atom and the N atom and $\bm{v}_{C_\alpha C}$ is a unit vector in the same direction as the bond between the C$_\alpha$ atom and the C atom, then the vectors $\bm{v}$ and $\bm{k}$ are:

$$\bm{v} = \bm{v}_{C_\alpha N} \times  \bm{v}_{C_\alpha C}, ~~ \bm{k} = \frac{\bm{v}_{C_\alpha C} -  \bm{v}_{C_\alpha N}}{||\bm{v}_{C_\alpha C} -  \bm{v}_{C_\alpha N}||}$$

The angle between a plane defined by vectors $\bm{v}$ and $\bm{k}$ and the vector pointing in the direction of C$_\beta$ atom, should be $~$150\degree, in order for the angles between any combination of 3 atoms (one of which has to be C$_\alpha$) are $~$110\degree.
After applying the rotational formula \ref{eq:rotformula}, scaling the unit vector $\bm{v_{rot}}$ by the length of the bond and adding the coordinates of the C$_\alpha$ atom the coordinate of the C$_\beta$ can be calculated.

Visual representation of the vectors and the operations can be seen on Figure \ref{fig:cbeta}.

\subsection{Gradient Descent Based Structure Optimization}

Gradient descent is method for gradual optimization of parameters of a model with a goal of decreasing some loss function. By model we understand a set of mathematical operations that take as an input the parameters and generate an output. 

In order to learn/optimize the structure, one requires some measure of how good the prediction matches the reality. In the second step of the pipeline we used a neural network to predict 32 bin distance histogram for each pair of amino acids together with auxiliary targets - torsion angles (36 bins for $\phi$ and 36 bins for $\psi$) and 8 class secondary structure. The distributions of distances and torsion angles were used in the structure optimization algorithm to define a loss function and to create several random initial states of structures by sampling from the torsion angle distributions.

Gradient descent is a widely used method in many machine learning algorithms, and the main optimization technique for Neural Networks (see Algorithm \ref{alg:gd}). The algorithm simply finds a direction in the space of parameters, in which the loss function decreases the fastest and moves in this direction. This is repeated many times until the process is stopped. Algorithm \ref{alg:strucreal_gd} sketches the steps of the optimization steps, which will be explained in the following paragraphs:

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{imgs_tomas/momentum.png}
    \caption{Gradient Descent with Momentum}
    \label{fig:momentum}
\end{figure}

\begin{algorithm}[ht]
\caption{Structure Optimization}
\label{alg:strucreal_gd}
\begin{algorithmic}[1]
\State Initialize $\bm{V}$
\State Random sample of model parameters $(\phi, \psi)$
    \Repeat
        \State Forward pass through the model $\mathcal{G}((\phi, \psi))$
        \State Calculate Loss
        \State Backward pass - Calculate Gradients ($\nabla (\phi, \psi)$)
        \State Normalize gradients
        \State Update $\bm{V}$: $\bm{V} = \mu \bm{V} - \alpha \nabla (\phi, \psi)$
        \State Update model parameters: $(\phi, \psi) = (\phi, \psi) + \bm{V}$
    \Until{i = iterations}
\end{algorithmic}
\end{algorithm}

In the first step we initialized a vector $\bm{V}$ which we used for implementing momentum. This is a simple idea the reuses gradient from previous iteration, where we no longer move in the direction of the steepest descent, but the direction is altered (dependent on the momentum parameter $\mu$) by adding the vector pointing in the direction of the last gradient. The principle of momentum is visualized on Figure \ref{fig:momentum}.

The second step refers to sampling of the torsion angles from the angle distributions. We fitted a von-Misses distribution to the histograms. Von Mises distribution is a continouous distribution defined on a circle in range $[-\pi, \pi)$ with a bell like shape. Formally its probability density function is 
$$\mathds{P} = \frac{e^{\kappa cos(x - \mu)}}{2\pi I_0(\kappa)}$$ 
where $I_0(\kappa) = \sum_{i=0}^\infty \frac{\kappa^{2i}}{2^{2i}(i!)^2}$ is a modified Bessel Function with order zero. 
The $\kappa$ parameter is called concentration ($\sim 1/\sigma^2$) and describes the dispersion and $\mu$ is the mean direction of the distribution \cite{vonmises}. 

In the next step the algorithm enters a cycle which ends after specified number iterations. In the first step of each iteration the model of protein geometry is created from current torsion angles ($(\phi, \psi)$), which are the parameters of the model, and the coordinates of backbone + C$_\beta$ atoms are computed. Finally a distance matrix is computed between all C$_\beta$ atoms of all residues (C$_\alpha$ for glycine).

We used the distance distributions predicted by the neural network to define a loss function:

\begin{equation}
     V_{dist}(\mathcal{G}((\phi, \psi))) = -\sum_{i, j, i \neq j} log[\mathds{P} (d_{ij} | S, MSA(S))]
     \label{eq:dist_pot}
\end{equation}

which is a simple negative log-likelihood function. In order to calculate the loss at any inter-residue distance, we approximated the distance histograms with a normal distribution (scaled, so that its max probability is the same as for the particular distogram).

The sixth step of the algorithm refers to a computation of the gradients, which thankfully is done automatically by pytorch by calling the \texttt{.backward} method. In order to avoid exploding gradients, we divided the gradients by their standard deviation before updating the parameters. This ensured that the gradients were roughly on the same scale throughout the algorithm interations, while preserving their orientations. In the 8th and 9th step the parameters are updated with the possibility of reusing previous gradients, by setting the momentum parameter $\mu > 0$.

\section{Evaluation Metrics}

In order to assess the performance of different models and most importantly, evaluate the closeness of the predictions to the reality a good metric is neccessary. The metrics used in protein folding protocols usually divide into two classes: alignment free and alignment dependent. As the name of the first category suggests, these metrics do not require for the two structures to be aligned while the latter ones does. The advantage of the alignment free metrics is that they are faster to use, however the alignment requiring metrics provide more information. 

We will first describe an alignment free metric we used for comparing and choosing between our models (DlDDT) and then the TM-Score metric, already mentioned in the introduction.

\subsection{(D)lDDT - (Distogram) local Distance Difference Test}

The lDDT metric evaluates the regions of distance matrices capturing local folding patterns. This means that only residues that are closer in reality than some threshold (usually $D_{ij} < 15$\AA) are compared. It is less affected by outliers than RMSD (Root Mean Squared Deviation) and correlates well with the global similarity measures.

The algorithm counts the number of inter-residue distances that are closer to the reality than another imposed threshold - it cycles through 4 different ones: $t \in \{0.5, 1, 2, 4\}$\AA \cite{lddt}.

The formula for comparing real distance matrix ($D$) and predicted one ($d$) for a domain of length $L$:

$$IDDT = \frac{100}{4} \sum_{t \in \{0.5, 1, 2, 4\}} \frac
{\sum_{i = 1}^L \sum_{j, |i - j| > 1, D_{ij} < 15} \mathds{1}(|D_{ij} - d_{ij}| < t)}
{\sum_{i = 1}^L \sum_{j, |i - j| > 1, D_{ij} < 15} 1} $$

However, since our models are essentialy classifiers and we predict a distribution of distances for each $D_{ij}$, Alphafold proposed a modified version of this metric. Instead of the indicator function in numerator it calculates probabilities of bins around the real distance that are closer to it than the treshold $t$. Formally:

\begin{equation}
DlDDT = \frac{100}{4} \sum_{t \in \{0.5, 1, 2, 4\}} \frac
{\sum_{i = 1}^L \sum_{j, |i - j| > 1, D_{ij} < 15} \mathds{P}(|D_{ij} - d_{ij}| < t~|~ \mathcal{S}, MSA)}
{\sum_{i = 1}^L \sum_{j, |i - j| > 1, D_{ij} < 15} 1}
    \label{eq:dlddt}
\end{equation}

\subsection{TM-score}



% The entire protein structure defined by the set of two torsion angles was derived by successfully applying the rotational formula (\ref{eq:rotformula}), with the first three atoms arbitrarily placed. The coordinate of C$_\beta$ atom (C$_\alpha$ for glycin) was used to compute the inter-residue distances.

% The protein geometry was implemented in \texttt{pytorch}, which means that the two torsion angles can be used as parameters to the model. \texttt{pytorch} allows for tracking the operations and builds a computational graph, which can then be efficiently backtracked and the gradients with regard to each parameter can be computed. 

% In order to learn/optimize the structure, one requires some measure of how good the prediction matches the reality. In the second of the pipeline we used a neural network to predict 32 bin distance histogram for each pair of amino acids torgether with auxiliary targets - torsion angles (36 bins for $\phi$ and 36 bins for $\psi$) and 8 class secondary structure.

% We used this distributions as a guide for the optimization process and defined a loss function:

% \begin{equation}
%     V(\phi, \psi) = V_{dist}(\mathcal{G}(\phi, \psi)) + V_{torsion}(\phi, \psi)
% \end{equation}

% where $ V_{dist}(\mathcal{G}(\phi, \psi))$ is a distance potential and $V_{torsion}(\phi, \psi)$ is torsion angle potential. Both of these quantities are essentialy negative log likelihood formulas:

% \begin{equation}
%      V_{dist}(\mathcal{G}(\phi, \psi)) = -\sum_{i, j, i \neq j} log[\mathds{P} (d_{ij} | S, MSA(S))]
% \end{equation}

% \begin{equation}
%     V_{torsion}(\phi, \psi) = -\sum_{i} log[\mathds{P}_{von Mises} (\phi, \psi | S, MSA(S))]
% \end{equation}

% which compute the log probability of a distribution at a certain value (\cite{alphafold} - supplementary information). In order for this to work properly, one has to work with smooth distributions (ie. derivatives can be calculated).

% \begin{figure}
%     \centering
%     \includegraphics[width=\linewidth]{imgs_tomas/norm_histograms.png}
%     \caption{Spline fitted distance histograms (middle) and von-Mises fitted $\phi$ angle distributions (right) in the red region displayed on the distance map on the left}
%     \label{fig:smooth_fits}
% \end{figure}

% We fitted a 3rd-degree spline to the distance histograms and von-Misses distribution \footnote{von Mises distribution is a continouous distribution defined on a circle in range $[-\pi, \pi)$ with a bell like shape. Formally its probability density function is $\mathds{P} = \frac{e^{\kappa cos(x - \mu)}}{2\pi I_0(\kappa)}$ where $I_0(\kappa) = \sum_{i=0}^\infty \frac{\kappa^{2i}}{2^{2i}(i!)^2}$ is a modified Bessel Function with order zero. 
% The $\kappa$ parameter is a called concentration and describes the dispersion. 
% $\mu$ is the mean direction of the distribution \cite{vonmises}} to the torsion angles distributions. 
% These two distributions were also used by the authors of the AlphaFold project \cite{alphafold}.

% Because there are so many possible protein conformations, the optimization is highly influenced by the initial conditions. 
% The torsion angles can be sampled from the predicted distributions and ideally the optimization process is run several times.  
        
% \subsection{Simplified case in 2D}
% Lets imagine the protein as a string of beads, where the beads right next to each other are constant distance away - lets call it $r$. 
% Any conformation of such string can be fully described by the signed successive polar angles - see Figure \ref{fig:struct_real2d}.
        
% \begin{figure}[ht]
%     \centering
%     \includegraphics[scale=0.22]{imgs_tomas/simplified_2d.png}
%     \caption{Caption}
%     \label{fig:struct_real2d}
% \end{figure}
        
% The left part of the figure introduces the notation when working with complex numbers. 
% The right part shows an arrangement of 5 residues in 2D space. 
% The grey-filled circles represent amino acid residues and the black vectors are the connections between residues of constant size $r$. 
% The first vector (connecting residue 0 and residue 1) is anchored at the origin. 
% The colorized vectors coming from the origin are translated versions of the remaining vectors. 
        
% In order to compute the distance between two residue, for example distance between residiue 0 and residue 4 (shown as a dotted line on the right hand side of the Figure \ref{fig:struct_real2d}) we need to add the contributions of every vector between those two points, ie:
        
% $$\bm{d_{0,4}} = \bm{d_{0,1}} + \bm{d_{1,2}} + \bm{d_{2,3}} + \bm{d_{3,4}}$$
        
% The magnitude of a complex number can be easily calculated from the Pythagorean formula:
        
% \begin{equation}
%     |z|^2 = x^2 + y^2 = cos^2(\theta) + sin^2(\theta)
%     \label{eq:complex_magnitude}
% \end{equation}
        
% The distance (in general) is:
        
% \begin{equation}
%     d_{i,j}^2 = r^2 |\bm{d_{0,1}} + \bm{d_{1,2}} + ... + \bm{d_{j-2,j-1}} + \bm{d_{j-1,j}}|^2
%     \label{eq:dist_gen}
% \end{equation}
        
% In complex representation (using both equation \ref{eq:complex_magnitude} and \ref{eq:dist_gen}):
        
% \begin{equation}
%     d_{i,j}^2 = r^2 \left|\sum_{k=i}^j cos(\theta_k) + i sin(\theta_k)\right|^2 = r^2 \left[\left(\sum_{k=i}^j cos(\theta_k)\right)^2 + \left(\sum_{k=i}^j sin(\theta_k)\right)^2\right]
%     \label{eq:dist_2d}
% \end{equation}
        
% \subsection{Simplified case: 3D}
        
% In 3D, the case is almost the same as in 2D, we just need use another angle ($\phi$) to fully describe the arrangement. 
% The transformation of Cartesian coordinates to polar is:
        
% $$x = r cos(\theta) sin(\phi)$$
% $$y = r sin(\theta) sin(\phi)$$
% $$z = r cos(\phi)$$
   
% The distance between two residues $i$ and $j$ in 3D is (just as in the 2D case) the magnitude of a vector pointing from residue $i$ to residue $j$. 
% This vector is again the sum of all vectors between these two residues. 
% Similarly to equation \ref{eq:dist_2d}, the formula for calculating (squared) distance between two residues in 3D is:
        
% \begin{equation}
%     d_{i,j}^2 = r^2 \left[\left(\sum_{k=i}^j cos(\theta_k) sin(\phi_k)\right)^2 + \left(\sum_{k=i}^j sin(\theta_k) sin(\phi_k)\right)^2 + \left(\sum_{k=i}^j cos(\phi_k)\right)^2\right]
% \end{equation}
        
% \subsection{Full Model}

